{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79233003",
   "metadata": {},
   "source": [
    "# WRAPPER METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e67e7",
   "metadata": {},
   "source": [
    "Wrapper methods treat feature selection as a search problem.\n",
    "They use a predictive model (like Logistic Regression, Decision Tree, or SVM) to test different subsets of features and evaluate performance (accuracy, F1 score, etc.).\n",
    "\n",
    "üëâ Unlike filter methods (independent of ML model), wrapper methods depend on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bfea4",
   "metadata": {},
   "source": [
    "Example to Understand\n",
    "\n",
    "Suppose you want to predict whether a person has diabetes or not using these features:\n",
    "\n",
    "- Age\n",
    "- BMI\n",
    "- Blood Pressure\n",
    "- Insulin Level\n",
    "- Glucose Level\n",
    "\n",
    "A wrapper method will:\n",
    "\n",
    "- Take different combinations of these features.\n",
    "- Train a model (say Logistic Regression).\n",
    "- Evaluate which combination gives the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511960c",
   "metadata": {},
   "source": [
    "## Main Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9fa08",
   "metadata": {},
   "source": [
    "### 1. Forward Selection\n",
    "\n",
    "Start with no features.\n",
    "\n",
    "Add one feature at a time (the one that improves model performance the most).\n",
    "\n",
    "Keep adding until no significant improvement.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "Step 1: Start with nothing ‚Üí add Glucose Level (highest accuracy).\n",
    "\n",
    "Step 2: Add BMI ‚Üí accuracy improves.\n",
    "\n",
    "Step 3: Add Blood Pressure ‚Üí no improvement ‚Üí stop.\n",
    "\n",
    "‚úÖ Final features = {Glucose, BMI}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6038f3d",
   "metadata": {},
   "source": [
    "### 2. Backward Elimination\n",
    "\n",
    "Start with all features.\n",
    "\n",
    "Remove the least important feature (based on performance drop).\n",
    "\n",
    "Repeat until removing features no longer improves accuracy.\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "Step 1: Start with {Age, BMI, BP, Insulin, Glucose}.\n",
    "\n",
    "Step 2: Remove Age (least effect).\n",
    "\n",
    "Step 3: Remove Insulin (least effect).\n",
    "\n",
    "‚úÖ Final features = {BMI, BP, Glucose}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4a6f2",
   "metadata": {},
   "source": [
    "### 3. Recursive Feature Elimination (RFE)\n",
    "\n",
    "Uses the model itself (like Logistic Regression or SVM) to rank features by importance.\n",
    "\n",
    "Removes the least important feature(s) step by step until only the desired number remains.\n",
    "\n",
    "üìå Example with RFE + Logistic Regression:\n",
    "\n",
    "Model ranks features ‚Üí {Glucose > BMI > BP > Insulin > Age}.\n",
    "\n",
    "Removes Age, then Insulin.\n",
    "\n",
    "Keeps {Glucose, BMI, BP}.\n",
    "\n",
    "‚ö° Most popular wrapper method because it‚Äôs systematic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cce349",
   "metadata": {},
   "source": [
    "### 4. Exhaustive Feature Selection (Brute Force)\n",
    "\n",
    "Tries all possible combinations of features.\n",
    "\n",
    "Picks the one with best performance.\n",
    "\n",
    "Very accurate but computationally expensive (not practical for large datasets).\n",
    "\n",
    "üìå Example:\n",
    "\n",
    "With 5 features ‚Üí 2‚Åµ = 32 subsets to check.\n",
    "\n",
    "Choose the best-performing subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cea6ce",
   "metadata": {},
   "source": [
    "# Code: Wrapper Methods with Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ab68a",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89664cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Selection\n",
    "forward_selector = SequentialFeatureSelector(\n",
    "    model, n_features_to_select=5, direction='forward'\n",
    ")\n",
    "forward_selector.fit(X_train, y_train)  # its only learning the best 5 features\n",
    "\n",
    "# Selected feature indices\n",
    "print(\"Forward Selection Features:\", forward_selector.get_support(indices=True))\n",
    "\n",
    "# Evaluate\n",
    "y_pred = forward_selector.transform(X_test)\n",
    "model.fit(forward_selector.transform(X_train), y_train)\n",
    "print(\"Forward Selection Accuracy:\", accuracy_score(y_test, model.predict(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca04009",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = forward_selector.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7edbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3383c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4faa6",
   "metadata": {},
   "source": [
    "## üîé Two Steps: fit() vs transform()\n",
    "\n",
    "### 1. forward_selector.fit(X_train, y_train)\n",
    "\n",
    "Here the search process happens.\n",
    "\n",
    "Forward selection goes like this:\n",
    "\n",
    "- Start with no features.\n",
    "- Train Logistic Regression with each single feature ‚Üí pick the one with best performance.\n",
    "- Add one more feature at a time, test all combinations, keep the best.\n",
    "- Repeat until 5 features are chosen.\n",
    "\n",
    "At the end of fit(), the selector remembers which 5 features were selected.\n",
    "\n",
    "‚ö° Important: Nothing in X_train changes yet ‚Äî the selector just figures out the \"winning subset\".\n",
    "\n",
    "### 2. forward_selector.transform(X_test)\n",
    "\n",
    "Now that the selector knows the 5 best features, transform() reduces the dataset to only those columns.\n",
    "\n",
    "If original data had 30 features ‚Üí transform() outputs only the selected 5 features.\n",
    "\n",
    "Example:\n",
    "\n",
    "`\n",
    "X_test.shape before: (114, 30)\n",
    "X_test.shape after transform: (114, 5)\n",
    "`\n",
    "\n",
    "### 3. Final Training & Evaluation\n",
    "\n",
    "`\n",
    "model.fit(forward_selector.transform(X_train), y_train)\n",
    "`\n",
    "\n",
    "We train Logistic Regression again, but now only on the 5 selected features (reduced dataset).\n",
    "\n",
    "`\n",
    "y_pred = model.predict(forward_selector.transform(X_test))\n",
    "`\n",
    "\n",
    "Predictions are also done using the reduced feature set.\n",
    "\n",
    "Accuracy is then calculated on this optimized dataset.\n",
    "\n",
    "üåü In Short\n",
    "\n",
    "- fit() ‚Üí searches and decides which features are best.\n",
    "- transform() ‚Üí actually applies that decision to shrink your dataset to the chosen features.\n",
    "\n",
    "Together = fit_transform() (common shortcut), but we separate them here because we need to fit on X_train and then transform X_test separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57146dfe",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Elimination\n",
    "backward_selector = SequentialFeatureSelector(\n",
    "    model, n_features_to_select=5, direction='backward'\n",
    ")\n",
    "backward_selector.fit(X_train, y_train) # its only learning the best 5 features \n",
    "\n",
    "# Selected feature indices\n",
    "print(\"Backward Elimination Features:\", backward_selector.get_support(indices=True))\n",
    "\n",
    "# Evaluate\n",
    "y_pred = backward_selector.transform(X_test)\n",
    "model.fit(backward_selector.transform(X_train), y_train)\n",
    "print(\"Backward Elimination Accuracy:\", accuracy_score(y_test, model.predict(y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb4b09",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e10f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train) # its only learning the best 5 features\n",
    "\n",
    "print(\"RFE Features:\", rfe.get_support(indices=True))\n",
    "\n",
    "# Evaluate\n",
    "model.fit(rfe.transform(X_train), y_train)\n",
    "y_pred = model.predict(rfe.transform(X_test))\n",
    "print(\"RFE Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb6b5d",
   "metadata": {},
   "source": [
    "## üîé Code Explanation\n",
    "\n",
    "### 1. Initializing RFE\n",
    "`\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "`\n",
    "\n",
    "- RFE = Recursive Feature Elimination.\n",
    "- model = Logistic Regression.\n",
    "- n_features_to_select=5 ‚Üí we want to keep only 5 features at the end.\n",
    "\n",
    "üëâ What happens behind the scenes:\n",
    "\n",
    "- RFE trains the model on all features.\n",
    "- The model provides a feature importance ranking (coefficients in Logistic Regression, or weights in SVM, or feature_importances_ in Tree-based models).\n",
    "- RFE removes the least important feature.\n",
    "- Then it retrains with the reduced set, removes the least important again‚Ä¶\n",
    "- Repeats this until only 5 features remain.\n",
    "\n",
    "So it‚Äôs like an iterative elimination tournament.\n",
    "\n",
    "### 2. Fitting on Training Data\n",
    "`\n",
    "rfe.fit(X_train, y_train)\n",
    "`\n",
    "\n",
    "Runs the RFE process:\n",
    "\n",
    "- Train model on all features.\n",
    "- Drop the least important one.\n",
    "- Retrain.\n",
    "- Repeat until 5 features are left.\n",
    "\n",
    "üëâ After this, rfe remembers which 5 features survived.\n",
    "\n",
    "### 3. Getting the Selected Features\n",
    "`\n",
    "print(\"RFE Features:\", rfe.get_support(indices=True))\n",
    "`\n",
    "\n",
    "get_support(indices=True) ‚Üí returns the column indices of the chosen 5 features.\n",
    "\n",
    "Example: [1, 7, 20, 25, 27].\n",
    "\n",
    "### 4. Training the Final Model\n",
    "`\n",
    "model.fit(rfe.transform(X_train), y_train)\n",
    "`\n",
    "\n",
    "rfe.transform(X_train) ‚Üí reduces training data to only those 5 selected features.\n",
    "\n",
    "Then Logistic Regression is trained on this reduced dataset.\n",
    "\n",
    "üëâ Now the model is simpler, using fewer features.\n",
    "\n",
    "### 5. Making Predictions & Accuracy\n",
    "`\n",
    "y_pred = model.predict(rfe.transform(X_test))\n",
    "`\n",
    "\n",
    "`\n",
    "print(\"RFE Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "`\n",
    "\n",
    "rfe.transform(X_test) ‚Üí keeps only the 5 selected features from the test set.\n",
    "\n",
    "Predictions are made with those features.\n",
    "\n",
    "Accuracy is compared against actual labels y_test.\n",
    "\n",
    "### üåü Analogy\n",
    "\n",
    "Think of RFE like eliminating weakest players step by step:\n",
    "\n",
    "Start with full cricket squad.\n",
    "\n",
    "Remove the weakest player based on performance.\n",
    "\n",
    "Play again, see who is now weakest, remove them.\n",
    "\n",
    "Repeat until only 5 best players remain.\n",
    "\n",
    "This is why it‚Äôs called Recursive Feature Elimination ‚Äî elimination happens step by step, recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63cfc3b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Exhaustive Feature Selection (‚ö†Ô∏è Heavy for many features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89710818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "\n",
    "# Exhaustive Feature Selection\n",
    "efs = ExhaustiveFeatureSelector(\n",
    "    model,\n",
    "    min_features=3,\n",
    "    max_features=5,\n",
    "    scoring='accuracy',\n",
    "    cv=3\n",
    ")\n",
    "efs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Exhaustive Selection Features:\", efs.best_idx_)\n",
    "\n",
    "# Evaluate\n",
    "model.fit(X_train[:, efs.best_idx_], y_train)\n",
    "y_pred = model.predict(X_test[:, efs.best_idx_])\n",
    "print(\"Exhaustive Selection Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312857f",
   "metadata": {},
   "source": [
    "| Method                                  | Best Situation to Use                                                                                                              | Why Choose It?                                                                            | When to Avoid                                                                                                |\n",
    "| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **Forward Selection**                   | Large dataset with **many features**, but you expect only a **few are useful**                                                     | Efficient, starts small and adds gradually. Works well when irrelevant features are many. | If there are **strong feature interactions** (two weak features together might be useful), it may miss them. |\n",
    "| **Backward Elimination**                | Dataset with a **moderate number of features** (‚â§ 20) and most features are likely useful                                          | Considers all features from the beginning, so captures feature interactions.              | Very slow if feature count is **large (100+)**, since it starts with everything.                             |\n",
    "| **Recursive Feature Elimination (RFE)** | You want a **systematic, model-based method**; especially good when model provides feature importance (LogReg, SVM, Decision Tree) | Widely used in practice, balances performance and interpretability, can rank features.    | Computationally expensive if dataset is huge (many features + many rows).                                    |\n",
    "| **Exhaustive Feature Selection**        | Very **small feature set (<15 features)** and you want the **absolute best subset**                                                | Tries all possible combinations, guarantees optimal result.                               | Becomes impossible for large feature sets (time grows exponentially: 2^n).                                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
